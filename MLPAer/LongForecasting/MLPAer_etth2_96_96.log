Args in experiment:
Namespace(random_seed=2025, is_training=1, model_id='ETTh2_96_96', model='MLPAer', data='ETTh2', root_path='./dataset/', data_path='ETTh2.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, fc_dropout=0.05, head_dropout=0.5, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=30, batch_size=128, patience=100, learning_rate=0.0001, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, c_ff=5, t_ff=128, c_dropout=0.1, t_dropout=0.1, embed_dropout=0.3, head_type='prediction', usenorm=1, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_96_96_MLPAer_ETTh2_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8449
val 2785
test 2785
Epoch: 1 cost time: 1.189953088760376
Epoch: 1, Steps: 66 | Train Loss: 0.5288498 Vali Loss: 0.2696236 Test Loss: 0.3527035
Validation loss decreased (inf --> 0.269624).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.9564793109893799
Epoch: 2, Steps: 66 | Train Loss: 0.4933757 Vali Loss: 0.2352910 Test Loss: 0.3150660
Validation loss decreased (0.269624 --> 0.235291).  Saving model ...
Updating learning rate to 0.0001
Epoch: 3 cost time: 0.8648040294647217
Epoch: 3, Steps: 66 | Train Loss: 0.4550725 Vali Loss: 0.2207581 Test Loss: 0.3023673
Validation loss decreased (0.235291 --> 0.220758).  Saving model ...
Updating learning rate to 0.0001
Epoch: 4 cost time: 0.864109992980957
Epoch: 4, Steps: 66 | Train Loss: 0.4424374 Vali Loss: 0.2167683 Test Loss: 0.2970222
Validation loss decreased (0.220758 --> 0.216768).  Saving model ...
Updating learning rate to 9e-05
Epoch: 5 cost time: 0.9286057949066162
Epoch: 5, Steps: 66 | Train Loss: 0.4339829 Vali Loss: 0.2154678 Test Loss: 0.2951141
Validation loss decreased (0.216768 --> 0.215468).  Saving model ...
Updating learning rate to 8.1e-05
Epoch: 6 cost time: 0.8581418991088867
Epoch: 6, Steps: 66 | Train Loss: 0.4314230 Vali Loss: 0.2130997 Test Loss: 0.2938271
Validation loss decreased (0.215468 --> 0.213100).  Saving model ...
Updating learning rate to 7.290000000000001e-05
Epoch: 7 cost time: 0.9192721843719482
Epoch: 7, Steps: 66 | Train Loss: 0.4282012 Vali Loss: 0.2140768 Test Loss: 0.2933762
EarlyStopping counter: 1 out of 100
Updating learning rate to 6.561e-05
Epoch: 8 cost time: 0.9109339714050293
Epoch: 8, Steps: 66 | Train Loss: 0.4263868 Vali Loss: 0.2136616 Test Loss: 0.2928286
EarlyStopping counter: 2 out of 100
Updating learning rate to 5.904900000000001e-05
Epoch: 9 cost time: 0.8942074775695801
Epoch: 9, Steps: 66 | Train Loss: 0.4246947 Vali Loss: 0.2134941 Test Loss: 0.2923029
EarlyStopping counter: 3 out of 100
Updating learning rate to 5.3144100000000005e-05
Epoch: 10 cost time: 0.9677929878234863
Epoch: 10, Steps: 66 | Train Loss: 0.4224237 Vali Loss: 0.2133388 Test Loss: 0.2919904
EarlyStopping counter: 4 out of 100
Updating learning rate to 4.782969000000001e-05
Epoch: 11 cost time: 0.9567978382110596
Epoch: 11, Steps: 66 | Train Loss: 0.4211008 Vali Loss: 0.2128418 Test Loss: 0.2914101
Validation loss decreased (0.213100 --> 0.212842).  Saving model ...
Updating learning rate to 4.304672100000001e-05
Epoch: 12 cost time: 0.9344804286956787
Epoch: 12, Steps: 66 | Train Loss: 0.4189377 Vali Loss: 0.2117025 Test Loss: 0.2914370
Validation loss decreased (0.212842 --> 0.211702).  Saving model ...
Updating learning rate to 3.874204890000001e-05
Epoch: 13 cost time: 0.9006764888763428
Epoch: 13, Steps: 66 | Train Loss: 0.4179225 Vali Loss: 0.2126214 Test Loss: 0.2911081
EarlyStopping counter: 1 out of 100
Updating learning rate to 3.486784401000001e-05
Epoch: 14 cost time: 0.956679105758667
Epoch: 14, Steps: 66 | Train Loss: 0.4168690 Vali Loss: 0.2119112 Test Loss: 0.2907145
EarlyStopping counter: 2 out of 100
Updating learning rate to 3.138105960900001e-05
Epoch: 15 cost time: 0.8559224605560303
Epoch: 15, Steps: 66 | Train Loss: 0.4159914 Vali Loss: 0.2111711 Test Loss: 0.2905493
Validation loss decreased (0.211702 --> 0.211171).  Saving model ...
Updating learning rate to 2.824295364810001e-05
Epoch: 16 cost time: 0.9281857013702393
Epoch: 16, Steps: 66 | Train Loss: 0.4140291 Vali Loss: 0.2114588 Test Loss: 0.2901914
EarlyStopping counter: 1 out of 100
Updating learning rate to 2.541865828329001e-05
Epoch: 17 cost time: 0.9207773208618164
Epoch: 17, Steps: 66 | Train Loss: 0.4142708 Vali Loss: 0.2110798 Test Loss: 0.2902128
Validation loss decreased (0.211171 --> 0.211080).  Saving model ...
Updating learning rate to 2.287679245496101e-05
Epoch: 18 cost time: 0.9313490390777588
Epoch: 18, Steps: 66 | Train Loss: 0.4129765 Vali Loss: 0.2119038 Test Loss: 0.2900107
EarlyStopping counter: 1 out of 100
Updating learning rate to 2.0589113209464907e-05
Epoch: 19 cost time: 0.891577959060669
Epoch: 19, Steps: 66 | Train Loss: 0.4121207 Vali Loss: 0.2107920 Test Loss: 0.2899102
Validation loss decreased (0.211080 --> 0.210792).  Saving model ...
Updating learning rate to 1.8530201888518416e-05
Epoch: 20 cost time: 0.8825733661651611
Epoch: 20, Steps: 66 | Train Loss: 0.4109319 Vali Loss: 0.2103442 Test Loss: 0.2897912
Validation loss decreased (0.210792 --> 0.210344).  Saving model ...
Updating learning rate to 1.6677181699666577e-05
Epoch: 21 cost time: 0.9892144203186035
Epoch: 21, Steps: 66 | Train Loss: 0.4105811 Vali Loss: 0.2109814 Test Loss: 0.2898609
EarlyStopping counter: 1 out of 100
Updating learning rate to 1.5009463529699919e-05
Epoch: 22 cost time: 0.8663735389709473
Epoch: 22, Steps: 66 | Train Loss: 0.4104214 Vali Loss: 0.2106778 Test Loss: 0.2896835
EarlyStopping counter: 2 out of 100
Updating learning rate to 1.3508517176729929e-05
Epoch: 23 cost time: 0.9387850761413574
Epoch: 23, Steps: 66 | Train Loss: 0.4101113 Vali Loss: 0.2116852 Test Loss: 0.2895810
EarlyStopping counter: 3 out of 100
Updating learning rate to 1.2157665459056936e-05
Epoch: 24 cost time: 0.8781554698944092
Epoch: 24, Steps: 66 | Train Loss: 0.4091756 Vali Loss: 0.2105712 Test Loss: 0.2895039
EarlyStopping counter: 4 out of 100
Updating learning rate to 1.0941898913151242e-05
Epoch: 25 cost time: 0.8777828216552734
Epoch: 25, Steps: 66 | Train Loss: 0.4082149 Vali Loss: 0.2101123 Test Loss: 0.2894860
Validation loss decreased (0.210344 --> 0.210112).  Saving model ...
Updating learning rate to 9.847709021836118e-06
Epoch: 26 cost time: 0.9118399620056152
Epoch: 26, Steps: 66 | Train Loss: 0.4084988 Vali Loss: 0.2103075 Test Loss: 0.2894865
EarlyStopping counter: 1 out of 100
Updating learning rate to 8.862938119652508e-06
Epoch: 27 cost time: 0.8702597618103027
Epoch: 27, Steps: 66 | Train Loss: 0.4080575 Vali Loss: 0.2112972 Test Loss: 0.2894082
EarlyStopping counter: 2 out of 100
Updating learning rate to 7.976644307687255e-06
Epoch: 28 cost time: 0.908534049987793
Epoch: 28, Steps: 66 | Train Loss: 0.4080585 Vali Loss: 0.2112583 Test Loss: 0.2893561
EarlyStopping counter: 3 out of 100
Updating learning rate to 7.178979876918531e-06
Epoch: 29 cost time: 0.8807203769683838
Epoch: 29, Steps: 66 | Train Loss: 0.4069700 Vali Loss: 0.2100073 Test Loss: 0.2893714
Validation loss decreased (0.210112 --> 0.210007).  Saving model ...
Updating learning rate to 6.4610818892266776e-06
Epoch: 30 cost time: 0.8809614181518555
Epoch: 30, Steps: 66 | Train Loss: 0.4077341 Vali Loss: 0.2108990 Test Loss: 0.2893048
EarlyStopping counter: 1 out of 100
Updating learning rate to 5.8149737003040096e-06
>>>>>>>testing : ETTh2_96_96_MLPAer_ETTh2_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.2893712818622589, mae:0.3402526378631592, rse:0.4297322928905487
